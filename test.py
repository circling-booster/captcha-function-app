# test_new.py
import sys
import os
import base64
import json
import torch
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from function_app import (
    ModelManager,
    preprocess_image,
    infer,
    ctc_decode,
    MODEL_CONFIGS,
    logger
)

def load_test_image(image_path: str) -> bytes:
    """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ"""
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    with open(image_path, 'rb') as f:
        image_bytes = f.read()
    
    logger.info(f"âœ“ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {image_path} ({len(image_bytes)} bytes)")
    return image_bytes

def test_inference_nol():
    """NOL ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    
    print("\n" + "="*60)
    print("ğŸ§ª Azure Function App - ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘ (NOL ëª¨ë¸)")
    print("="*60 + "\n")
    
    model_type = "melon"
    test_img_dir = project_root / "test_img"
    
    # test_img ë””ë ‰í† ë¦¬ í™•ì¸
    if not test_img_dir.exists():
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {test_img_dir}")
        print(f"   ìƒì„±í•˜ë ¤ë©´: mkdir {test_img_dir}")
        return
    
    # test_img ë””ë ‰í† ë¦¬ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_files = list(test_img_dir.glob("*.png")) + list(test_img_dir.glob("*.jpg")) + list(test_img_dir.glob("*.jpeg"))
    
    if not image_files:
        print(f"âŒ test_img ë””ë ‰í† ë¦¬ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        print(f"   ì§€ì› í˜•ì‹: .png, .jpg, .jpeg")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ ì´ë¯¸ì§€ íŒŒì¼: {len(image_files)}ê°œ")
    
    for image_file in image_files:
        print(f"\n{'â”€'*60}")
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {image_file.name}")
        print(f"{'â”€'*60}")
        
        try:
            # 1. ì´ë¯¸ì§€ ë¡œë“œ
            print(f"\n[1/5] ì´ë¯¸ì§€ ë¡œë“œ ì¤‘...")
            image_bytes = load_test_image(str(image_file))
            
            # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            print(f"[2/5] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘... [{model_type}]")
            image_tensor = preprocess_image(image_bytes, model_type)
            print(f"âœ“ í…ì„œ shape: {image_tensor.shape}")
            print(f"  - ë°°ì¹˜ í¬ê¸°: {image_tensor.shape[0]}")
            print(f"  - ì±„ë„: {image_tensor.shape[1]}")
            print(f"  - ë†’ì´: {image_tensor.shape[2]}")
            print(f"  - ë„ˆë¹„: {image_tensor.shape[3]}")
            
            # 3. ëª¨ë¸ ë¡œë“œ
            print(f"[3/5] ëª¨ë¸ ë¡œë“œ ì¤‘... [{model_type}]")
            model = ModelManager.get_model(model_type)
            device = ModelManager.get_device()
            print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"  - ëª¨ë¸ íƒ€ì…: {model_type}")
            print(f"  - ë””ë°”ì´ìŠ¤: {device}")
            print(f"  - ì´ë¯¸ì§€ í¬ê¸°: {MODEL_CONFIGS[model_type]['width']}x{MODEL_CONFIGS[model_type]['height']}")
            
            # 4. ì¶”ë¡  ìˆ˜í–‰
            print(f"[4/5] ì¶”ë¡  ìˆ˜í–‰ ì¤‘...")
            predicted_texts = infer(image_tensor, model, device)
            predicted_text = predicted_texts[0] if predicted_texts else ""
            print(f"âœ“ ì¶”ë¡  ê²°ê³¼: {predicted_text}")
            
            # 5. ì‹ ë¢°ë„ ê³„ì‚°
            print(f"[5/5] ì‹ ë¢°ë„ ê³„ì‚° ì¤‘...")
            with torch.no_grad():
                image_tensor_device = image_tensor.to(device)
                logits = model(image_tensor_device)
                probs = torch.softmax(logits, dim=2)
                confidence = float(probs.max().item())
                max_prob_idx = probs.max(dim=2)[1]
            
            print(f"âœ“ ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.2f}%)")
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼")
            print(f"{'='*60}")
            print(f"ì´ë¯¸ì§€: {image_file.name}")
            print(f"ëª¨ë¸: {model_type}")
            print(f"ì¸ì‹ í…ìŠ¤íŠ¸: {predicted_text}")
            print(f"ì‹ ë¢°ë„: {confidence:.4f}")
            print(f"{'='*60}\n")
            
        except FileNotFoundError as e:
            print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

def test_health_check():
    """í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¥ í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    try:
        model_status = {}
        for model_type in MODEL_CONFIGS.keys():
            try:
                model = ModelManager.get_model(model_type)
                model_status[model_type] = {
                    "status": "loaded",
                    "last_load_time": str(ModelManager._last_load_time.get(model_type, "N/A"))
                }
                print(f"âœ“ {model_type}: ë¡œë“œë¨")
            except Exception as e:
                model_status[model_type] = {
                    "status": "not_loaded",
                    "error": str(e)
                }
                print(f"âŒ {model_type}: {e}")
        
        device = ModelManager.get_device()
        print(f"\në””ë°”ì´ìŠ¤: {device}")
        print(f"ìƒíƒœ: healthy")
    except Exception as e:
        print(f"âŒ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("\nğŸš€ Azure Function App ë¡œì»¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    # í—¬ìŠ¤ ì²´í¬ ì‹¤í–‰
    test_health_check()
    
    # NOL ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_inference_nol()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
